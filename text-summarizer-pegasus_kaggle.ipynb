{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-25T07:03:30.103416Z","iopub.execute_input":"2024-06-25T07:03:30.104314Z","iopub.status.idle":"2024-06-25T07:03:31.133864Z","shell.execute_reply.started":"2024-06-25T07:03:30.104280Z","shell.execute_reply":"2024-06-25T07:03:31.132927Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install transformers[sentencepiece] datasets sacrebleu rouge_score py7zr -q","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install rouge_score","metadata":{"execution":{"iopub.status.busy":"2024-06-25T09:39:46.846041Z","iopub.execute_input":"2024-06-25T09:39:46.846714Z","iopub.status.idle":"2024-06-25T09:40:01.991251Z","shell.execute_reply.started":"2024-06-25T09:39:46.846681Z","shell.execute_reply":"2024-06-25T09:40:01.990088Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge_score) (1.16.0)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=c8b8e81b2d1c98c5c31028f847913b209ae1e9f094b80e71582623151c371d9a\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install --upgrade accelerate\n!pip uninstall -y transformers accelerate\n!pip install transformers accelerate","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import pipeline, set_seed\nfrom datasets import load_dataset, load_from_disk\nimport matplotlib.pyplot as plt\nfrom datasets import load_dataset\nimport pandas as pd\nfrom datasets import load_dataset, load_metric\n\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\nimport nltk\nfrom nltk.tokenize import sent_tokenize\n\nfrom tqdm import tqdm\nimport torch\n\nnltk.download(\"punkt\")\n\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\nfrom transformers import DataCollatorForSeq2Seq\nfrom transformers import TrainingArguments, Trainer\n","metadata":{"execution":{"iopub.status.busy":"2024-06-25T07:03:31.135620Z","iopub.execute_input":"2024-06-25T07:03:31.136093Z","iopub.status.idle":"2024-06-25T07:03:50.083566Z","shell.execute_reply.started":"2024-06-25T07:03:31.136066Z","shell.execute_reply":"2024-06-25T07:03:50.082602Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-06-25T07:03:50.084767Z","iopub.execute_input":"2024-06-25T07:03:50.085409Z","iopub.status.idle":"2024-06-25T07:03:50.114582Z","shell.execute_reply.started":"2024-06-25T07:03:50.085381Z","shell.execute_reply":"2024-06-25T07:03:50.113584Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"code","source":"model_ckpt = \"google/pegasus-cnn_dailymail\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n\nmodel_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)\n\n#dowload & unzip data\n\n!wget https://github.com/entbappy/Branching-tutorial/raw/master/summarizer-data.zip\n!unzip summarizer-data.zip","metadata":{"execution":{"iopub.status.busy":"2024-06-25T07:03:53.442921Z","iopub.execute_input":"2024-06-25T07:03:53.443615Z","iopub.status.idle":"2024-06-25T07:04:18.098649Z","shell.execute_reply.started":"2024-06-25T07:03:53.443583Z","shell.execute_reply":"2024-06-25T07:04:18.097493Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/88.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c33b64fb964b4e54b4fbfe051ade8506"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.12k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"86c65994ab1e4976b73e6bc5bd03a52f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/1.91M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bc1b2e99f924b26b1323b5204d73a62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f3ba192bffd4fc783029a5aecc0aced"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.28G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5055cb989354b1bacaf709cd969a7c0"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/280 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0d0fb9b106a45a7b6643f8f54d40a0d"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"--2024-06-25 07:04:16--  https://github.com/entbappy/Branching-tutorial/raw/master/summarizer-data.zip\nResolving github.com (github.com)... 140.82.116.4\nConnecting to github.com (github.com)|140.82.116.4|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://raw.githubusercontent.com/entbappy/Branching-tutorial/master/summarizer-data.zip [following]\n--2024-06-25 07:04:16--  https://raw.githubusercontent.com/entbappy/Branching-tutorial/master/summarizer-data.zip\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 7903594 (7.5M) [application/zip]\nSaving to: 'summarizer-data.zip'\n\nsummarizer-data.zip 100%[===================>]   7.54M  --.-KB/s    in 0.06s   \n\n2024-06-25 07:04:16 (132 MB/s) - 'summarizer-data.zip' saved [7903594/7903594]\n\nArchive:  summarizer-data.zip\n  inflating: samsum-test.csv         \n  inflating: samsum-train.csv        \n  inflating: samsum-validation.csv   \n   creating: samsum_dataset/\n extracting: samsum_dataset/dataset_dict.json  \n   creating: samsum_dataset/test/\n  inflating: samsum_dataset/test/data-00000-of-00001.arrow  \n  inflating: samsum_dataset/test/dataset_info.json  \n  inflating: samsum_dataset/test/state.json  \n   creating: samsum_dataset/train/\n  inflating: samsum_dataset/train/data-00000-of-00001.arrow  \n  inflating: samsum_dataset/train/dataset_info.json  \n  inflating: samsum_dataset/train/state.json  \n   creating: samsum_dataset/validation/\n  inflating: samsum_dataset/validation/data-00000-of-00001.arrow  \n  inflating: samsum_dataset/validation/dataset_info.json  \n  inflating: samsum_dataset/validation/state.json  \n","output_type":"stream"}]},{"cell_type":"code","source":"dataset_samsum = load_from_disk('samsum_dataset')\ndataset_samsum","metadata":{"execution":{"iopub.status.busy":"2024-06-25T07:04:18.101751Z","iopub.execute_input":"2024-06-25T07:04:18.102154Z","iopub.status.idle":"2024-06-25T07:04:18.133032Z","shell.execute_reply.started":"2024-06-25T07:04:18.102113Z","shell.execute_reply":"2024-06-25T07:04:18.132144Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 14732\n    })\n    test: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 819\n    })\n    validation: Dataset({\n        features: ['id', 'dialogue', 'summary'],\n        num_rows: 818\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"split_lengths = [len(dataset_samsum[split])for split in dataset_samsum]\n\nprint(f\"Split lengths: {split_lengths}\")\nprint(f\"Features: {dataset_samsum['train'].column_names}\")\nprint(\"\\nDialogue:\")\n\nprint(dataset_samsum[\"test\"][1][\"dialogue\"])\n\nprint(\"\\nSummary:\")\n\nprint(dataset_samsum[\"test\"][1][\"summary\"])","metadata":{"execution":{"iopub.status.busy":"2024-06-25T07:04:18.134188Z","iopub.execute_input":"2024-06-25T07:04:18.134784Z","iopub.status.idle":"2024-06-25T07:04:18.146122Z","shell.execute_reply.started":"2024-06-25T07:04:18.134753Z","shell.execute_reply":"2024-06-25T07:04:18.145090Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Split lengths: [14732, 819, 818]\nFeatures: ['id', 'dialogue', 'summary']\n\nDialogue:\nEric: MACHINE!\nRob: That's so gr8!\nEric: I know! And shows how Americans see Russian ;)\nRob: And it's really funny!\nEric: I know! I especially like the train part!\nRob: Hahaha! No one talks to the machine like that!\nEric: Is this his only stand-up?\nRob: Idk. I'll check.\nEric: Sure.\nRob: Turns out no! There are some of his stand-ups on youtube.\nEric: Gr8! I'll watch them now!\nRob: Me too!\nEric: MACHINE!\nRob: MACHINE!\nEric: TTYL?\nRob: Sure :)\n\nSummary:\nEric and Rob are going to watch a stand-up on youtube.\n","output_type":"stream"}]},{"cell_type":"code","source":"def convert_examples_to_features(example_batch):\n    input_encodings = tokenizer(example_batch['dialogue'] , max_length = 1024, truncation = True )\n    \n    with tokenizer.as_target_tokenizer():\n        target_encodings = tokenizer(example_batch['summary'], max_length = 128, truncation = True )\n        \n    return {'input_ids' : input_encodings['input_ids'],\n            'attention_mask': input_encodings['attention_mask'],\n            'labels': target_encodings['input_ids']}","metadata":{"execution":{"iopub.status.busy":"2024-06-25T07:05:08.903320Z","iopub.execute_input":"2024-06-25T07:05:08.903805Z","iopub.status.idle":"2024-06-25T07:05:08.910209Z","shell.execute_reply.started":"2024-06-25T07:05:08.903765Z","shell.execute_reply":"2024-06-25T07:05:08.909233Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"dataset_samsum_pt = dataset_samsum.map(convert_examples_to_features, batched = True)\ndataset_samsum_pt[\"train\"]","metadata":{"execution":{"iopub.status.busy":"2024-06-25T07:05:09.170317Z","iopub.execute_input":"2024-06-25T07:05:09.171221Z","iopub.status.idle":"2024-06-25T07:05:13.728284Z","shell.execute_reply.started":"2024-06-25T07:05:09.171171Z","shell.execute_reply":"2024-06-25T07:05:13.727385Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/14732 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9db3d9682bb6405aa08e802ecd44e7ae"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/819 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c2ad3df14d24090a7189acddf8577cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/818 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"167d817cbcb74fc1b36c5f9b6b61dd60"}},"metadata":{}},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n    num_rows: 14732\n})"},"metadata":{}}]},{"cell_type":"code","source":"# Training\nseq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)\ntrainer_args = TrainingArguments(output_dir='pegasus-samsum', \n                                 num_train_epochs=1, \n                                 warmup_steps=500,\n                                 per_device_train_batch_size=1, \n                                 per_device_eval_batch_size=1,\n                                 weight_decay=0.01, \n                                 logging_steps=10,\n                                 evaluation_strategy='steps', \n                                 eval_steps=500, \n                                 save_steps=1e6,\n                                 gradient_accumulation_steps=16)","metadata":{"execution":{"iopub.status.busy":"2024-06-25T07:05:13.729750Z","iopub.execute_input":"2024-06-25T07:05:13.730024Z","iopub.status.idle":"2024-06-25T07:05:13.761505Z","shell.execute_reply.started":"2024-06-25T07:05:13.730001Z","shell.execute_reply":"2024-06-25T07:05:13.760624Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"trainer = Trainer(model=model_pegasus, \n                  args=trainer_args,\n                  tokenizer=tokenizer, \n                  data_collator=seq2seq_data_collator,\n                  train_dataset=dataset_samsum_pt[\"train\"], \n                  eval_dataset=dataset_samsum_pt[\"validation\"])\n\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-25T08:00:23.014618Z","iopub.execute_input":"2024-06-25T08:00:23.014978Z","iopub.status.idle":"2024-06-25T08:40:54.720078Z","shell.execute_reply.started":"2024-06-25T08:00:23.014950Z","shell.execute_reply":"2024-06-25T08:40:54.719108Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='920' max='920' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [920/920 40:28, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>1.295600</td>\n      <td>1.433179</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=920, training_loss=1.4378034177033798, metrics={'train_runtime': 2430.9575, 'train_samples_per_second': 6.06, 'train_steps_per_second': 0.378, 'total_flos': 5528248038285312.0, 'train_loss': 1.4378034177033798, 'epoch': 0.9991854466467553})"},"metadata":{}}]},{"cell_type":"code","source":"trainer_args = TrainingArguments(output_dir='pegasus-samsum', \n                                 num_train_epochs=10, \n                                 warmup_steps=500,\n                                 per_device_train_batch_size=1, \n                                 per_device_eval_batch_size=1,\n                                 weight_decay=0.01, \n                                 logging_steps=10,\n                                 eval_strategy='steps', \n                                 eval_steps=500, \n                                 save_steps=1e6,\n                                 gradient_accumulation_steps=16)\n\ntrainer = Trainer(model=model_pegasus, \n                  args=trainer_args,\n                  tokenizer=tokenizer, \n                  data_collator=seq2seq_data_collator,\n                  train_dataset=dataset_samsum_pt[\"train\"], \n                  eval_dataset=dataset_samsum_pt[\"validation\"])\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-25T09:47:10.784075Z","iopub.execute_input":"2024-06-25T09:47:10.784907Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='42' max='9200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  42/9200 01:46 < 6:46:07, 0.38 it/s, Epoch 0.04/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"# Evaluation\ndef generate_batch_sized_chunks(list_of_elements, batch_size):\n    \"\"\"split the dataset into smaller batches that we can process simultaneously\n    Yield successive batch-sized chunks from list_of_elements.\"\"\"\n    for i in range(0, len(list_of_elements), batch_size):\n        yield list_of_elements[i : i + batch_size]","metadata":{"execution":{"iopub.status.busy":"2024-06-25T09:39:13.534535Z","iopub.execute_input":"2024-06-25T09:39:13.535370Z","iopub.status.idle":"2024-06-25T09:39:13.542837Z","shell.execute_reply.started":"2024-06-25T09:39:13.535335Z","shell.execute_reply":"2024-06-25T09:39:13.541638Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def calculate_metric_on_test_ds(dataset, metric, model, tokenizer, batch_size=16, device=device, column_text=\"article\", column_summary=\"highlights\"):\n    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))\n    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n\n    for article_batch, target_batch in tqdm(zip(article_batches, target_batches), total=len(article_batches)):\n        inputs = tokenizer(article_batch, \n                           max_length=1024,  \n                           truncation=True, \n                           padding=\"max_length\", \n                           return_tensors=\"pt\")\n        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device), \n                                   attention_mask=inputs[\"attention_mask\"].to(device), \n                                   length_penalty=0.8, \n                                   num_beams=8, \n                                   max_length=128)\n        ''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''\n        \n        # Finally, we decode the generated texts, replace the  token, and add the decoded texts with the references to the metric.\n        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, clean_up_tokenization_spaces=True) for s in summaries]\n        decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n    #  Finally compute and return the ROUGE scores.\n    score = metric.compute()\n    return score","metadata":{"execution":{"iopub.status.busy":"2024-06-25T09:39:14.670207Z","iopub.execute_input":"2024-06-25T09:39:14.671033Z","iopub.status.idle":"2024-06-25T09:39:14.681012Z","shell.execute_reply.started":"2024-06-25T09:39:14.670992Z","shell.execute_reply":"2024-06-25T09:39:14.680099Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"rouge_names = [\"rouge1\", \"rouge2\", \"rougeL\", \"rougeLsum\"]\nrouge_metric = load_metric('rouge')","metadata":{"execution":{"iopub.status.busy":"2024-06-25T09:40:01.994605Z","iopub.execute_input":"2024-06-25T09:40:01.994930Z","iopub.status.idle":"2024-06-25T09:40:02.275544Z","shell.execute_reply.started":"2024-06-25T09:40:01.994897Z","shell.execute_reply":"2024-06-25T09:40:02.274436Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/datasets/load.py:759: FutureWarning: The repository for rouge contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.2/metrics/rouge/rouge.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"score = calculate_metric_on_test_ds(dataset_samsum['test'][0:10], rouge_metric, trainer.model, tokenizer, batch_size = 2, column_text = 'dialogue', column_summary= 'summary')\n\nrouge_dict = dict((rn, score[rn].mid.fmeasure ) for rn in rouge_names )\npd.DataFrame(rouge_dict, index = [f'pegasus'] )","metadata":{"execution":{"iopub.status.busy":"2024-06-25T09:40:04.951626Z","iopub.execute_input":"2024-06-25T09:40:04.952348Z","iopub.status.idle":"2024-06-25T09:40:12.371379Z","shell.execute_reply.started":"2024-06-25T09:40:04.952316Z","shell.execute_reply":"2024-06-25T09:40:12.370439Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"100%|██████████| 5/5 [00:07<00:00,  1.43s/it]\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"           rouge1  rouge2    rougeL  rougeLsum\npegasus  0.024794     0.0  0.024838   0.024809","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rouge1</th>\n      <th>rouge2</th>\n      <th>rougeL</th>\n      <th>rougeLsum</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>pegasus</th>\n      <td>0.024794</td>\n      <td>0.0</td>\n      <td>0.024838</td>\n      <td>0.024809</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"## Save model\nmodel_pegasus.save_pretrained(\"pegasus-samsum-model\")","metadata":{"execution":{"iopub.status.busy":"2024-06-25T09:40:12.373543Z","iopub.execute_input":"2024-06-25T09:40:12.373943Z","iopub.status.idle":"2024-06-25T09:40:16.432264Z","shell.execute_reply.started":"2024-06-25T09:40:12.373904Z","shell.execute_reply":"2024-06-25T09:40:16.431139Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\nNon-default generation parameters: {'max_length': 128, 'min_length': 32, 'num_beams': 8, 'length_penalty': 0.8, 'forced_eos_token_id': 1}\n","output_type":"stream"}]},{"cell_type":"code","source":"## Save tokenizer\ntokenizer.save_pretrained(\"tokenizer\")","metadata":{"execution":{"iopub.status.busy":"2024-06-25T09:40:16.433594Z","iopub.execute_input":"2024-06-25T09:40:16.433913Z","iopub.status.idle":"2024-06-25T09:40:16.471501Z","shell.execute_reply.started":"2024-06-25T09:40:16.433884Z","shell.execute_reply":"2024-06-25T09:40:16.470454Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"('tokenizer/tokenizer_config.json',\n 'tokenizer/special_tokens_map.json',\n 'tokenizer/spiece.model',\n 'tokenizer/added_tokens.json',\n 'tokenizer/tokenizer.json')"},"metadata":{}}]},{"cell_type":"code","source":"pwd","metadata":{"execution":{"iopub.status.busy":"2024-06-25T09:43:52.579976Z","iopub.execute_input":"2024-06-25T09:43:52.580760Z","iopub.status.idle":"2024-06-25T09:43:52.592465Z","shell.execute_reply.started":"2024-06-25T09:43:52.580727Z","shell.execute_reply":"2024-06-25T09:43:52.591595Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working'"},"metadata":{}}]},{"cell_type":"code","source":"#Load\n# tokenizer = AutoTokenizer.from_pretrained(\"/content/tokenizer\")","metadata":{"execution":{"iopub.status.busy":"2024-06-25T09:40:16.473695Z","iopub.execute_input":"2024-06-25T09:40:16.474670Z","iopub.status.idle":"2024-06-25T09:40:16.899250Z","shell.execute_reply.started":"2024-06-25T09:40:16.474634Z","shell.execute_reply":"2024-06-25T09:40:16.897611Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Prediction\n\ngen_kwargs = {\"length_penalty\": 0.8, \"num_beams\":8, \"max_length\": 128}\nsample_text = dataset_samsum[\"test\"][0][\"dialogue\"]\nreference = dataset_samsum[\"test\"][0][\"summary\"]\npipe = pipeline(\"summarization\", model=\"pegasus-samsum-model\",tokenizer=tokenizer)\n\n## \nprint(\"Dialogue:\")\nprint(sample_text)\n\nprint(\"\\nReference Summary:\")\nprint(reference)\n\nprint(\"\\nModel Summary:\")\nprint(pipe(sample_text, **gen_kwargs)[0][\"summary_text\"])","metadata":{"execution":{"iopub.status.busy":"2024-06-25T09:42:05.389296Z","iopub.execute_input":"2024-06-25T09:42:05.390106Z","iopub.status.idle":"2024-06-25T09:42:22.944689Z","shell.execute_reply.started":"2024-06-25T09:42:05.390066Z","shell.execute_reply":"2024-06-25T09:42:22.943665Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"Your max_length is set to 128, but your input_length is only 122. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=61)\n","output_type":"stream"},{"name":"stdout","text":"Dialogue:\nHannah: Hey, do you have Betty's number?\nAmanda: Lemme check\nHannah: <file_gif>\nAmanda: Sorry, can't find it.\nAmanda: Ask Larry\nAmanda: He called her last time we were at the park together\nHannah: I don't know him well\nHannah: <file_gif>\nAmanda: Don't be shy, he's very nice\nHannah: If you say so..\nHannah: I'd rather you texted him\nAmanda: Just text him 🙂\nHannah: Urgh.. Alright\nHannah: Bye\nAmanda: Bye bye\n\nReference Summary:\nHannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n\nModel Summary:\nAmanda can't find Betty's number. Larry called Betty the last time they were at the park together. Hannah wants Amanda to text Larry instead.\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}